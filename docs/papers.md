# Papers

This document aims to list some useful papers to read for someone working on this problem

## Mesh

* [DeepMesh](https://www.semanticscholar.org/paper/DeepMeSH%3A-deep-semantic-representation-for-MeSH-Peng-You/fe1818e57cbd3823c7fbfaa577b21a232a43dbd5)
* [MeshProbeNet](https://pubmed.ncbi.nlm.nih.gov/30851089/)
* [MeshLabeler](https://pubmed.ncbi.nlm.nih.gov/26072501/)
* [AttentionMesh](https://users.cs.duke.edu/~bdhingra/papers/attentionmesh.pdf)
* [BertMesh](https://pubmed.ncbi.nlm.nih.gov/32976559/)

# Extreme multilabel classification

* [DeepXML](https://dl.acm.org/doi/pdf/10.1145/3077136.3080834)
* [Parabel](http://manikvarma.org/pubs/prabhu18b.pdf)
* [Pecos](https://arxiv.org/pdf/2010.05878.pdf)
* [AttentionXML](https://arxiv.org/abs/1811.01727)
* [XBert](https://assets.amazon.science/e3/f2/2ec101df490caf28a3e596289a53/x-bert-extreme-multi-label-text-classification-using-bidirectional-encoder-representations-from-transformers.pdf)

## Tools or techniques

* [BioBERT](https://pubmed.ncbi.nlm.nih.gov/31501885/)
* [Snapshot ensembles](https://arxiv.org/abs/1704.00109v1)
* [Threshold optimisation](https://pralab.diee.unica.it/sites/default/files/pillai_PR2013_Thresholding_0.pdf)
* [Bag of tricks for efficient text classification](https://aclanthology.org/E17-2068/)
* [Revisiting small batch training](https://arxiv.org/abs/1804.07612)
* [BioSent2Vec](https://arxiv.org/abs/1810.09302)
* [Dont decay the learning rate, increase the batch size](https://arxiv.org/abs/1711.00489v2)
* [Self attentive sentence embedding](https://arxiv.org/pdf/1703.03130.pdf)
