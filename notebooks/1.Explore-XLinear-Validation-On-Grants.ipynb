{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff1194d-4e3e-43e2-a511-f8a8ac0a1519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import output_notebook, output_file, reset_output, show, output_file\n",
    "from bokeh.models import ColumnDataSource\n",
    "from wellcomeml.viz.palettes import Wellcome11, WellcomeBackground\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "from grants_tagger.predict import predict_tags\n",
    "from grants_tagger.utils import load_data\n",
    "\n",
    "from datascience.grants import cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c245968-0c31-4044-a889-8abc5e917de8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load grants file and tag with approach x-linear\n",
    "\n",
    "We load the grants.csv file, which is the one with all grants (no filter), and then predict with xlinear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2541b5a-7cd1-4a66-bf67-2f56b6486588",
   "metadata": {},
   "outputs": [],
   "source": [
    "grants = pd.read_csv('../data/raw/grants.csv', index_col=0)\n",
    "grants = cleaning.clean_grants(grants).fillna('')\n",
    "\n",
    "predictions = pd.read_csv('../data/interim/mesh_pipeline_result.csv')\n",
    "predictions.rename({'Grant id': 'Grant ID'}, axis=1, inplace=True)\n",
    "\n",
    "merged_predictions_metadata = pd.merge(left=predictions, right=grants, how='left', on='Grant ID')\n",
    "merged_predictions_metadata = merged_predictions_metadata[merged_predictions_metadata['Start Date'] > '2012']\n",
    "merged_predictions_metadata[::-1].to_csv('../data/processed/merged_mesh_predictions_mesh_xlinear_for_validation.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14bee02-60fc-4079-af58-c35c41e0d51f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Check average tags per threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d813f-ca0d-4730-aaab-d201ce1aac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for th in tqdm.tqdm(np.arange(0.0, 1, 0.001)):\n",
    "    # Trims based on threshold\n",
    "    subset = merged_predictions_metadata[merged_predictions_metadata['Prob'] >= th]\n",
    "    \n",
    "    subset = subset.groupby('Grant ID').agg({'Tag': 'count'})['Tag']\n",
    "    \n",
    "    avg = subset.mean()\n",
    "    std = subset.std()\n",
    "    median = subset.median()\n",
    "    \n",
    "    results.append(\n",
    "        {'th': th, 'avg': avg, 'std': std, 'median': median, '95': subset.quantile(0.95), '05': subset.quantile(0.05)}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da8f93-188b-4b86-9333-c8bd02bbce0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Plot a nice graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d4e45-5779-43ef-8860-c30173f3db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file('../figures/threshold_per_average_html.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8490752-d69e-4933-b464-de5fecae7f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(title=\"Average Tags per label\", background_fill_color=\"#fafafa\", width=800, height=400)\n",
    "\n",
    "x = [r['th'] for r in results]\n",
    "y = [r['avg'] for r in results]\n",
    "yplus = [r['avg']+2*r['std'] for r in results]\n",
    "\n",
    "yminus = [max(r['avg']-2*r['std'], 0) for r in results]\n",
    "p.line(x, y, line_color=str(Wellcome11[0]), alpha=1)\n",
    "\n",
    "p.varea(x, y1=yminus, y2=yplus, alpha=0.3)\n",
    "\n",
    "p.background_fill_color = str(WellcomeBackground)\n",
    "p.xaxis[0].ticker = np.arange(0.0, 1, 0.05)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('x', '$x{0.0000f}'), ('y', '$y')], mode='vline'))\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2dddc2-0c35-4f13-89de-281696941ac1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Calculate precision-recall on publications\n",
    "\n",
    "For the train-test split used during training, we calculate precision-recall. At this point, we're not yet using grants data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95240c2e-5e77-4133-aa22-b320301a6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '../data/processed/train_mesh2021.jsonl'\n",
    "test_data_path = '../data/processed/test_mesh2021.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863644f7-7644-4884-8d68-34b84aab4c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y, _ = load_data(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f517d-babf-4b69-951e-b37089a068c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(set([tag for tags in y for tag in tags]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5b8d5-307c-40b0-8c22-8be644d41cca",
   "metadata": {},
   "source": [
    "## 5. Load merged prediction files and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce23532-6f4a-4886-9b40-6cc069b0c7ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_predictions = pd.read_excel('../data/processed/merged_mesh_predictions_mesh_xlinear_for_validation.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a1c48e-e5b7-4913-9b45-14f06f413621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_predictions['Tag'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038cbca-6cec-4c23-b729-9c32825ac88c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(merged_predictions['Prob'], bins=150, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63686b1-bc6a-4c4d-b179-ec017ba46795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"Probability density\", background_fill_color=\"#fafafa\", width=800, height=400)\n",
    "\n",
    "x = bins\n",
    "y = hist\n",
    "\n",
    "p.vbar(x=x, top=y, line_color=str(Wellcome11[0]), width=0.005)\n",
    "\n",
    "p.background_fill_color = str(WellcomeBackground)\n",
    "p.xaxis[0].ticker = np.arange(0.0, 1, 0.05)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('x', '$x{0.0000f}'), ('y', '$y')], mode='vline'))\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c99e2dc-d212-44fd-99cf-747490b08935",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Load grants annotated by Wellcome\n",
    "\n",
    "Now we load the many validation files generated by people at Wellcome and calculate \"accuracy\" metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d516a4-f486-4623-aaf2-b96eb960cb74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in glob.glob('../data/raw/mesh_for_validation_2022/*.xlsx'):\n",
    "    df = pd.read_excel(file)\n",
    "    df.rename(columns={'0/1/2/?': 'Annotation', '0/1/2/3/?': 'Annotation'}, inplace=True)\n",
    "    df.dropna(subset=['Annotation'], inplace=True)\n",
    "    dfs.append(df[['Grant ID', 'MeSH Tag', 'MeSH Chapter', 'Prob', 'Annotation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ee677-72f4-435e-ab63-ce364d646d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "concatenated = pd.concat(dfs)\n",
    "\n",
    "print(f\"A total of {len(concatenated)} tags have been verified\")\n",
    "print(f\"Unique annotations: {concatenated['Annotation'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6723d042-f409-485d-bc32-aded15a6dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = concatenated[concatenated['Annotation'].isin([0, 1, 2, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a479e0-22f7-4b09-83ff-917293ad9f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_relevant = (concatenated['Annotation'] > 0).mean()\n",
    "average_primary = (concatenated['Annotation'] == 1).mean()\n",
    "average_secondary = (concatenated['Annotation'] >= 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c982836-f23f-4427-b164-980de5caace6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Precision {average_relevant}. Primary: {average_primary}, Secondary: {average_secondary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4980d3c3-95ec-41b0-a883-771f630f4238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_pos, bins_pos = np.histogram(\n",
    "    concatenated[(concatenated['Annotation'] > 0)]['Prob'], bins=50, density=True\n",
    ")\n",
    "\n",
    "hist_neg, bins_neg = np.histogram(\n",
    "    concatenated[(concatenated['Annotation'] == 0)]['Prob'], bins=50, density=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3a26b-a340-4b9b-826b-45d1c1f7fd7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_file('density.html')\n",
    "            \n",
    "p = figure(title=\"Probability density\", background_fill_color=\"#fafafa\", width=800, height=400)\n",
    "\n",
    "\n",
    "p.line(x=bins_pos[1:], y=hist_pos, line_color=str(Wellcome11[0]), width=2)\n",
    "p.line(x=bins_neg[1:], y=hist_neg, line_color=str(Wellcome11[1]), width=2)\n",
    "\n",
    "p.background_fill_color = str(WellcomeBackground)\n",
    "p.xaxis[0].ticker = np.arange(0.0, 1, 0.05)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('x', '$x{0.0000f}'), ('y', '$y')], mode='vline'))\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322f7c5-4b59-4cf1-a060-556b44e793b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a2aba-8069-4817-a3f8-3c43484c031c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpr, tpr, th_roc = roc_curve(y_true=concatenated['Annotation'] > 0, y_score=concatenated['Prob'])\n",
    "precision, recall, th_pr_rc = precision_recall_curve(y_true=concatenated['Annotation'] > 0, probas_pred=concatenated['Prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071fb0de-3104-4515-ae95-977a53dc8b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = figure(title=\"With a 0.01 threshold, 80% of the tags are correct\", background_fill_color=\"#fafafa\", width=800, height=400)\n",
    "\n",
    "source = {\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'th': th_pr_rc\n",
    "}\n",
    "p.line(x='recall', y='precision', source=source, line_color=str(Wellcome11[0]), width=2)\n",
    "\n",
    "p.background_fill_color = str(WellcomeBackground)\n",
    "p.xaxis[0].ticker = np.arange(0.0, 1, 0.05)\n",
    "\n",
    "p.add_tools(HoverTool(tooltips=[('precision', '@precision'), ('recall', '@recall'), ('th', '@th')], mode='vline'))\n",
    "\n",
    "p.xaxis.axis_label=\"% of tags output by the model\"\n",
    "\n",
    "p.yaxis.axis_label=\"% of correct tags\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b283a2-d93b-4a27-b04a-5fb411eb6db7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for th in [0.1, 0.2, 0.5, 0.8, 0.9]:\n",
    "    i = np.argmax(th < th_pr_rc)\n",
    "    print(f\"{th} {precision[i]:.2f} {recall[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b7c0b-e753-4254-b382-f7aff815260d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.argmax(0.1 < th_pr_rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642880c-46e5-415b-9a83-5d1b273eee68",
   "metadata": {},
   "source": [
    "### 6. Precision recall per tree-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c30175-6eb0-4ffe-b21d-b822cf902ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datascience.mesh import load_mesh_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e709cb-dbbb-4443-8f4d-cb933a6b8ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_to_id, id_to_string = load_mesh_tree(path_to_tree='../data/raw/desc2021.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7186e7-12dc-42f5-a36a-c5aaba8d9a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_to_min_tree_level = {\n",
    "    string: min([len(idx.split('.')) for idx in mesh_ids]) for string, mesh_ids in string_to_id.items()\n",
    "    if len(mesh_ids) > 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6e62d-a8ad-4bd7-bfd9-c5d1654faddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_level = []\n",
    "for i in [1, 2, 3, 4, 5]:\n",
    "    concatenated_level[i] = concatenated[concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5fe405-7843-43fb-b6fa-756988fa0f87",
   "metadata": {},
   "source": [
    "## 7. Explore research proposal predictions\n",
    "\n",
    "We will answer the questions:\n",
    "\n",
    "1. How many grants have \"Research Question\" ?\n",
    "\n",
    "Only very few. (7063)\n",
    "\n",
    "2. How many predictions differ by considering \"Research Question\" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84104c16-0b6e-47b2-9fe6-4a9eb73d2521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "research_question_predictions = pd.read_csv('../data/interim/mesh_pipeline_result_research_question.csv')\n",
    "standard_fields_predictions = pd.read_csv('../data/interim/mesh_pipeline_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435ef54-78be-462c-9497-de7c320340c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "research_question_predictions['Grant id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a26131-ede9-4db2-bcbb-e69fb4b51721",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_that_have_research_question = research_question_predictions['Grant id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ec0f4-9efa-4aeb-be7a-8fc721fd271d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags_standard_prediction = standard_fields_predictions[standard_fields_predictions['Grant id'].isin(ids_that_have_research_question)].groupby('Grant id').agg(\n",
    "    {'Tag': set}\n",
    ")\n",
    "\n",
    "tags_research_question = research_question_predictions[research_question_predictions['Grant id'].isin(ids_that_have_research_question)].groupby('Grant id').agg(\n",
    "    {'Tag': set}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b8979-a59a-4a22-aeb0-02ae3e099b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comparison_df = tags_standard_prediction.join(tags_research_question, on='Grant id', lsuffix='_standard', rsuffix='_rq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8656b7e-3a70-4251-b9bf-01a5d8bbe3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set({1,2}).intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabdb93-9ed5-411d-b71f-0ebb7df3e681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iou = comparison_df[['Tag_standard', 'Tag_rq']].apply(\n",
    "    lambda x: len(x['Tag_standard'].union(x['Tag_rq']))-len(x['Tag_standard']), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753842d2-b74f-4177-8e25-95a586348e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iou.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grants_tagger",
   "language": "python",
   "name": "grants_tagger"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
