prepare_data:
    years: 2018,2019
    test_years: 2020,2021

train:
    learning_rate: 1e-4
    batch_size: 64 # with nn.DataParallel otherwise 8
    epochs: 1
    pretrained_model: microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract
    multilabel_attention: False
